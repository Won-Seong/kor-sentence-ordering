{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOtjX3fdl4kRUpD4jGNs8BU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Set Up"],"metadata":{"id":"ss-NVDjbXD6D"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H3oECCoFWxeF"},"outputs":[],"source":["import sys\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","drive_path = os.path.join('drive', 'MyDrive', 'Colab Notebooks', 'Dacon', 'SentenceOrder') # 다른 드라이브에서 사용할 경우, 이 부분만 적절히 수정\n","\n","if drive_path not in sys.path:\n","  sys.path.append(drive_path)\n","from utility import *\n","\n","SEED = 42\n","set_all_seed(SEED)\n","\n","CONFIG_PATH = drive_path + '/config.yaml'\n","config = load_config(CONFIG_PATH)"]},{"cell_type":"code","source":["with open(drive_path + '/dataset/first_augmented_data.pkl', 'rb') as f:\n","  first_augmented_data = pickle.load(f) # 첫 번째 증강 데이터\n","with open(drive_path + '/dataset/second_augmented_data.pkl', 'rb') as f:\n","  second_augmented_data = pickle.load(f) # 두 번째 증강 데이터\n","train = pd.read_csv(drive_path + '/dataset/train.csv').drop(columns = 'ID') # 훈련 데이터"],"metadata":{"id":"CoKSfWQrXGmh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create DataFrame from the Augmented Data"],"metadata":{"id":"kAxHogtLXK3N"}},{"cell_type":"code","source":["# 증강 데이터 세트에서 오류가 있는 데이터 파악\n","# 예를 들어, 문장이 네 개가 아니거나 한자 및 특수 문자를 포함한 데이터는 오류로 판단\n","\n","first_error_idx = make_error_idx(first_augmented_data)\n","second_error_idx = make_error_idx(second_augmented_data)"],"metadata":{"id":"CPMtqeCiXQia"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 오류 데이터 제외한 데이터를 이용하여 데이터 프레임 생성\n","\n","first_df =  make_df_from_complete_text_list(first_augmented_data, first_error_idx)\n","second_df = make_df_from_complete_text_list(second_augmented_data, second_error_idx)"],"metadata":{"id":"oNH2h2CJXTjz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 두 데이터 프레임을 합친 증강 데이터 프레임 생성\n","\n","augmented_df = pd.concat([first_df, second_df]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n","augmented_df.to_csv(drive_path + '/dataset/augmented_df.csv', index=False)\n","augmented_df.head(5)"],"metadata":{"id":"5uJs2jLwXXog"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create DataFrame from the Original Dataset"],"metadata":{"id":"KXRg0f3KXh51"}},{"cell_type":"code","source":["# 오리지널 데이터 세트로부터 데이터 세트 생성\n","# 새 데이터 세트에는 오리지널 데이터가 정렬된 상태\n","\n","sentences = [[] for _ in range(4)]\n","sentence_order = []\n","\n","for i, row in tqdm(train.iterrows()):\n","  for j in range(4):\n","    sentences[j].append(row.iloc[row[f'answer_{j}']])"],"metadata":{"id":"FBB4fZ7fXnJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 정렬된 오리지널 데이터 프레임 생성\n","\n","ordered_train = make_df(sentences)\n","ordered_train.to_csv(drive_path + '/dataset/ordered_train.csv', index=False)\n","ordered_train.head(5)"],"metadata":{"id":"P6IsQrZkXq_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the Joint Dataset"],"metadata":{"id":"lXbKOuc0XsBa"}},{"cell_type":"code","source":["# 증강 데이터와 오리지널 데이터를 합친 조인트 데이터 프레임 생성\n","\n","joint_df = pd.concat([ordered_train, augmented_df]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n","joint_df.to_csv(drive_path + '/dataset/joint_df.csv', index=False)\n","joint_df.head(5)"],"metadata":{"id":"1-xNdG5sXt_Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 각 데이터 프레임의 길이 출력\n","\n","print( \"Length of the Original Dataset:\", len(train) )\n","print( \"Length of the Augmented Dataset\", len(augmented_df) )\n","print( \"Length of the Final Dataset:\", len(joint_df) )"],"metadata":{"id":"abmkLPY5YMkz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Create the Randomized Final Dataset\n"],"metadata":{"id":"DzrY83H3BSpx"}},{"cell_type":"code","source":["def create_randomized_dataset(df, random_numbers, sentences, answers):\n","  # 정렬된 네 개의 문장을 섞어 데이터를 생성하는 함수\n","\n","  for _, row in tqdm(df.iterrows()):\n","    random.shuffle(random_numbers)\n","    for i, n in enumerate(random_numbers):\n","      sentences[i].append(row.iloc[n]) # 이 배열에는 무작위로 섞인 네 개의 문장을 저장\n","      answers[n].append(i) # 이 배열에는 정답 저장"],"metadata":{"id":"K50XGCIsCGOq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_numbers = [0, 1, 2, 3] # 무작위로 섞일 문장의 순서이자 정답이 될 배열\n","sentences = [[] for _ in range(4)] # 네 개의 무작위로 섞인 문장이 담길 배열\n","answers = [[] for _ in range(4)] # 정답이 담길 배열\n","\n","for _ in range(6): # 전체 데이터에 대해, 무작위 데이터 생성 6번 반복\n","  create_randomized_dataset(joint_df, random_numbers, sentences, answers)"],"metadata":{"id":"JZpyWuPCDXYy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["final_train_df = make_df(sentences, answers).drop_duplicates().sample(frac=1, random_state=SEED).reset_index(drop=True) # 무작위로 섞인 데이터에서 중복 데이터를 제거하여 데이터 프레임 생성\n","final_train_df.to_csv(drive_path + '/dataset/final_train_df.csv', index=False) # 최종 데이터 프레임 저장\n","final_train_df.head(5)\n","print(\"Length of the Final Train Dataset: \", len(final_train_df))"],"metadata":{"id":"XlHgnuUFEeXL"},"execution_count":null,"outputs":[]}]}