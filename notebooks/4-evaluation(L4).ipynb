{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"L4","authorship_tag":"ABX9TyNUttUn+uSwnApPZhodZnHC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Set Up"],"metadata":{"id":"LOEEO0mRjEV0"}},{"cell_type":"code","source":["%%capture\n","import os\n","if \"COLAB_\" not in \"\".join(os.environ.keys()):\n","    !pip install unsloth\n","else:\n","    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n","    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n","    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n","    !pip install --no-deps unsloth"],"metadata":{"id":"mcM2ccd2vkAt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","from google.colab import drive\n","import os\n","\n","drive.mount('/content/drive')\n","drive_path = os.path.join('drive', 'MyDrive', 'Colab Notebooks', 'Dacon', 'SentenceOrder') # 다른 드라이브에서 사용할 경우, 이 부분만 적절히 수정\n","\n","if drive_path not in sys.path:\n","  sys.path.append(drive_path)\n","from utility import *\n","\n","SEED=42\n","CONFIG_PATH = drive_path + '/config.yaml'\n","\n","set_all_seed(SEED)\n","config = load_config(CONFIG_PATH)"],"metadata":{"id":"weVK_XYDiu6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test = pd.read_csv(drive_path + '/dataset/test.csv').drop(columns = 'ID')"],"metadata":{"id":"qBIq5OlAjGkS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["inference_message = [\n","    {\"role\": \"system\", \"content\": \"You are an expert at understanding the logical flow of sentences. Your task is to arrange four given Korean sentences into a coherent and natural paragraph. Output the ordered sequence of sentence indices, separated by commas. /no_think\"},\n","    {\"role\": \"user\", \"content\": \"Provided Sentences:\\n0. {sentence_0}\\n1. {sentence_1}\\n2. {sentence_2}\\n3. {sentence_3}\"}\n","]"],"metadata":{"id":"Ckk6npn4jP1E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluate\n","\n","문장의 순서를 맞추는 작업에서는 창의성이 필요하지 않으므로 do_sample을 False로 두어 확률이 제일 높은 토큰을 생성"],"metadata":{"id":"YgYPmpVrjkvB"}},{"cell_type":"code","source":["def evaluation(model, tokenizer, row, chat_template):\n","  # 하나의 테스트 데이터를 모델이 추론\n","\n","  input = tokenizer(\n","      [ formatting_prompts(row, chat_template) ],\n","      return_tensors=\"pt\"\n","  ).to(\"cuda\")\n","\n","  outputs = model.generate(\n","      input_ids=input.input_ids,\n","      attention_mask=input.attention_mask,\n","      eos_token_id=tokenizer.eos_token_id,\n","      max_new_tokens=32,\n","      use_cache=True,\n","      do_sample=False,\n","      temperature=None,\n","      top_p=None,\n","      top_k=None,\n","      min_p=None,\n","      num_beams=1,\n","  )\n","  response = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","  ans = list(map(int, response[0][-10:].split(','))) # 다른 텍스트를 전부 제외한, 순수 정답만 추출한 후 리스트로 변환\n","\n","  return ans"],"metadata":{"id":"LRdftuk0jj9-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_list = [1200, 1400, 2400] # 추론에 사용할 모델의 체크 포인트 리스트"],"metadata":{"id":"0XqUSp43njJS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for check_point in checkpoint_list:\n","  # 체크 포인트 리스트에 있는 모든 체크 포인트에 대해, 모델을 불러온 후 추론하여 그 결과를 csv 파일로 저장\n","\n","  sample_submission = pd.read_csv(drive_path + '/dataset/sample_submission.csv')\n","\n","  model, tokenizer = FastLanguageModel.from_pretrained(\n","    model_name = drive_path + f'/models/qwen_14b_finetuning-r32-alpha32-unsloth/checkpoint-{check_point}',\n","    max_seq_length = 512,\n","    load_in_4bit = True,\n","  )\n","\n","  model.eval()\n","  FastLanguageModel.for_inference(model)\n","\n","  inference_chat_template = tokenizer.apply_chat_template(inference_message, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n","\n","  for i, row in tqdm(test.iterrows()):\n","    ans = evaluation(model, tokenizer, row, inference_chat_template)\n","    for j in range(4):\n","      sample_submission.iloc[i, 1 + j] = ans[j]\n","  sample_submission.to_csv(drive_path + f'/prediction/qwen_14b_finetuning-r32-alpha32-unsloth-{check_point}.csv')\n","\n","  del model\n","  del tokenizer\n","  torch.cuda.empty_cache()\n","  gc.collect()"],"metadata":{"id":"KFnz07Hld3Z7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save the Best Model"],"metadata":{"id":"btaHNdnmikx1"}},{"cell_type":"code","source":["# 제일 높은 성능을 가진 모델을 Hugging Face Model Hub에 저장\n","# 환경 변수에 HF_TOKEN을 설정해야 저장이 가능\n","# os.environ[\"HF_TOKEN\"] = \"hf_xxxxxxxxx\"\n","\n","check_point = 1200\n","\n","model, tokenizer = FastLanguageModel.from_pretrained(\n","  model_name = drive_path + f'/models/qwen_14b_finetuning-r32-alpha32-unsloth/checkpoint-{check_point}',\n","  max_seq_length = 512,\n","  load_in_4bit = True,\n",")\n","\n","new_model_name = \"Qwen-3-14B-Sentence-Ordering\"\n","model.push_to_hub(new_model_name)\n","tokenizer.push_to_hub(new_model_name)"],"metadata":{"id":"qR0ZiHXzimeA"},"execution_count":null,"outputs":[]}]}